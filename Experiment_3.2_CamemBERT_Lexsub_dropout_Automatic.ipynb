{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-14T09:29:41.878815Z",
     "start_time": "2024-05-14T09:29:41.871865Z"
    }
   },
   "outputs": [],
   "source": [
    "#!git clone https://github.com/jvladika/Lexical-Substitution.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "12d5d2c990307dbb",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-14T12:24:13.101128Z",
     "start_time": "2024-05-14T12:23:37.390282Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\stefa\\anaconda3\\envs\\Thesis\\Lib\\site-packages\\transformers\\utils\\generic.py:260: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "Some weights of the model checkpoint at camembert-base were not used when initializing CamembertForMaskedLM: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "- This IS expected if you are initializing CamembertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing CamembertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, CamembertModel, CamembertForMaskedLM\n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.mixture import GaussianMixture\n",
    "import warnings\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from french_lefff_lemmatizer.french_lefff_lemmatizer import FrenchLefffLemmatizer as fll\n",
    "import torch\n",
    "import string\n",
    "import nltk\n",
    "import time\n",
    "import numpy as np\n",
    "# process takes time, we use tqdm to show progress\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "# Cuda\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('camembert-base')\n",
    "lm_model = CamembertForMaskedLM.from_pretrained('camembert-base').to(device)\n",
    "raw_model = CamembertModel.from_pretrained('camembert-base', output_hidden_states=True, output_attentions=True).to(device)\n",
    "def load_transformers():\n",
    "    return tokenizer, lm_model, raw_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "70a6a28cb34345b2",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-14T12:24:21.093345Z",
     "start_time": "2024-05-14T12:24:21.065283Z"
    }
   },
   "outputs": [],
   "source": [
    "# imports from filter\n",
    "def filter_substitutions(substitutions):\n",
    "    dels = list()\n",
    "    for sub in substitutions:\n",
    "        if sub.lower() in substitutions and sub.capitalize() in substitutions:\n",
    "            dels.append(sub.capitalize())\n",
    "        if sub.lower() in substitutions and sub.upper() in substitutions:\n",
    "            dels.append(sub.upper())\n",
    "        if sub in nltk.corpus.stopwords.words('french') or sub in string.punctuation:\n",
    "            dels.append(sub)\n",
    "    dels = list(set(dels))\n",
    "    for d in dels:\n",
    "        substitutions.remove(d)\n",
    "    return substitutions\n",
    "\n",
    "def filter_words(target, words, scores, tokens):\n",
    "    # lets time\n",
    "    time_filter = time.time()\n",
    "    dels = list()\n",
    "    toks = tokens.tolist()\n",
    "    blacklist = [target, target.capitalize()]\n",
    "    \n",
    "    for w in words:\n",
    "        if w.lower() in words and w.capitalize() in words:\n",
    "            dels.append(w.capitalize())\n",
    "        if w.lower() in words and w.upper() in words:\n",
    "            dels.append(w.upper())\n",
    "        if w in nltk.corpus.stopwords.words('french') or w in string.punctuation:\n",
    "            dels.append(w)\n",
    "        if w in blacklist:\n",
    "            dels.append(w)\n",
    "    \n",
    "\n",
    "    dels = list(set(dels))\n",
    "    for d in dels:\n",
    "        del scores[words.index(d)]\n",
    "        del toks[words.index(d)]\n",
    "        words.remove(d)\n",
    "    \n",
    "\n",
    "    return words, scores, torch.tensor(toks)\n",
    "\n",
    "\n",
    "# imports from scores\n",
    "\n",
    "#Calculates the similarity score\n",
    "def similarity_score(original_output, subst_output, k):\n",
    "    mask_idx = k\n",
    "    cos_sim = torch.nn.CosineSimilarity(dim=1, eps=1e-6)\n",
    "    weights = torch.div(torch.stack(list(original_output[3])).squeeze().sum(0).sum(0), (12 * 12.0))\n",
    "\n",
    "    #Calculate the similarittimey score \n",
    "    #SIM(x, x'; k) = sum_i^L [ w_{i,k} * cos(h(x_i|x), h(x_i'|x')) ]\n",
    "\n",
    "    #subst_output = raw_model(sent.reshape(1, sent.shape[0]))\n",
    "    suma = 0.0\n",
    "    sent_len = original_output[2][2].shape[1]\n",
    "\n",
    "    for token_idx in range(sent_len):     \n",
    "        original_hidden = original_output[2]\n",
    "        subst_hidden = subst_output[2]\n",
    "\n",
    "        #Calculate the contextualized representation of the i-th word as a concatenation of RoBERTa's values in its last four layers\n",
    "        context_original = torch.cat( tuple( [original_hidden[hs_idx][:, token_idx, :] for hs_idx in [1, 2, 3, 4]] ), dim=1)\n",
    "        context_subst = torch.cat( tuple( [subst_hidden[hs_idx][:, token_idx, :] for hs_idx in [1, 2, 3, 4]] ), dim=1)\n",
    "        suma += weights[mask_idx][token_idx] * cos_sim(context_original, context_subst)\n",
    "\n",
    "    substitute_validation = suma\n",
    "    return substitute_validation\n",
    "\n",
    "\n",
    "#Calculates the proposal score\n",
    "def proposal_score(original_score, subst_scores):\n",
    "    subst_scores = torch.tensor(subst_scores)\n",
    "    # we have to revert original_score to cpu\n",
    "    original_score = original_score.cpu()\n",
    "    return np.log(torch.div(subst_scores , (1.0 - original_score)) )\n",
    "\n",
    "\n",
    "#finals, props, subval = calc_scores(scores, input_ids[i], input_embeds[i], original_score, mask_position)\n",
    "def calc_scores(scr, sentences, original_output, original_score, mask_index):\n",
    "    #Get representations of all substitute sentences\n",
    "    sentences= torch.tensor(sentences).to(device)\n",
    "    subst_output = raw_model(sentences)\n",
    "\n",
    "    prop_score = proposal_score(original_score, scr) # this is cpu\n",
    "    substitute_validation = similarity_score(original_output, subst_output, mask_index)\n",
    "\n",
    "    alpha = 0.003\n",
    "    # Move prop_score to the same device as substitute_validation before the operation\n",
    "    prop_score = prop_score.to(substitute_validation.device)\n",
    "\n",
    "    final_score = substitute_validation + alpha*prop_score\n",
    "    \n",
    "    '''\n",
    "    print(\"Proposal score: \" + str(prop_score))\n",
    "    print(\"Subst. validation: \" + str(substitute_validation))\n",
    "    print(\"Final score for \" + str(final_score) + \"\\n\")\n",
    "    '''\n",
    "    return final_score, prop_score, substitute_validation\n",
    "\n",
    "WSD_PATTERN = r' (\\w+)/\\w+\\.\\w\\.\\d+' # (Woord), letterlijke slash, woord, letterlijke punt, letter, cijfer(s)\n",
    "\n",
    "def preproc_sentence(sentence):\n",
    "    sent_preproc = re.sub(WSD_PATTERN, r'\\1', sentence) # Alleen woord blijft over\n",
    "    return sent_preproc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cf443f5c962246d3",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-14T12:24:23.002063Z",
     "start_time": "2024-05-14T12:24:22.978910Z"
    }
   },
   "outputs": [],
   "source": [
    "def lexsub_dropout(sentence, target, topk=15):\n",
    "    sentence = sentence.replace('-', ' ')\n",
    "    table = str.maketrans(dict.fromkeys(string.punctuation)) \n",
    "\n",
    "    #Remove unnecessary punctuation from the sentence (such as: \"GET *free food *coupons!!\")\n",
    "    split_sent = nltk.word_tokenize(sentence)\n",
    "    split_sent = list(map(lambda wrd : wrd.translate(table) if wrd not in string.punctuation else wrd, split_sent))\n",
    "    original_sent = ' '.join(split_sent)\n",
    "\n",
    "    #Get raw model word embeddings for words in the sentence\n",
    "    original_token_input = tokenizer.encode(\" \"+original_sent, return_tensors=\"pt\").to(device)\n",
    "    original_output = raw_model(original_token_input)\n",
    "    inputs_embeds = original_output[2][1]\n",
    "\n",
    "    #The target word to substitute\n",
    "    target_token_id = tokenizer.encode(\" \"+target)[1]\n",
    "    input_ids = tokenizer.encode(\" \" + original_sent)\n",
    "    \n",
    "    mask_position = input_ids.index(target_token_id)\n",
    "    #input_ids = torch.tensor(input_ids).to(device)\n",
    "    #Set a percentage of randomly selected embedding weights of the target word to 0.\n",
    "    embedding_dim = 768\n",
    "    dropout_percent = 0.3\n",
    "    dropout_amount = round(dropout_percent*embedding_dim)\n",
    "\n",
    "    #Start timing the experiment.\n",
    "    start = time.time()\n",
    "\n",
    "    #Run multiple experiments and then take average because of stochastic nature of choosing indices to dropout (sometimes the predictions are gibberish)\n",
    "    all_scores = dict()\n",
    "    all_counts = dict()\n",
    "    num_iterations = 5\n",
    "    for it in range(num_iterations):\n",
    "        #Choose the weight indices to drop out.\n",
    "        dropout_indices = np.random.choice(embedding_dim, dropout_amount, replace=False)\n",
    "        #Apply dropout to the embeddings\n",
    "        inputs_embeds[0, mask_position, dropout_indices] = 0\n",
    "        #Pass the embeddings where masked word's embedding is partially droppped out to the model \n",
    "        with torch.no_grad():\n",
    "                output = lm_model(inputs_embeds=inputs_embeds)\n",
    "        logits = output[0].squeeze()\n",
    "        #Get top guesses\n",
    "        mask_logits = logits[mask_position]\n",
    "        top_tokens = torch.topk(mask_logits, k=16, dim=0)[1]\n",
    "        scores = torch.softmax(mask_logits, dim=0)[top_tokens].tolist()\n",
    "        words = [tokenizer.decode(i.item()).strip() for i in top_tokens]\n",
    "        words, scores, top_tokens = filter_words(target, words, scores, top_tokens) # THIS IS WHAT MAKES IT SLOW\n",
    "        assert len(words) == len(scores)\n",
    "        if len(words) == 0: \n",
    "            continue\n",
    "\n",
    "        #Calculate proposal scores, substitute validation scores, and final scores\n",
    "        original_score = torch.softmax(mask_logits, dim=0)[target_token_id]\n",
    "        sentences = list()\n",
    "        for i in range(len(words)):\n",
    "            subst_word = top_tokens[i]\n",
    "            input_ids[mask_position] = int(subst_word)\n",
    "            sentences.append(list(input_ids))\n",
    "       \n",
    "        finals, props, subval = calc_scores(scores, sentences, original_output, original_score, mask_position)\n",
    "        finals = map(lambda f : float(f), finals)\n",
    "        props = map(lambda f : float(f), props)\n",
    "        subval = map(lambda f : float(f), subval)\n",
    "\n",
    "        if target in words:\n",
    "            words.remove(target)\n",
    "\n",
    "        #Update total scores and counts in the dictionary\n",
    "        res = dict(zip(words, finals))\n",
    "        for w, s in res.items():\n",
    "            all_scores[w] = all_scores[w] + s if w in all_scores.keys() else s\n",
    "            all_counts[w] = all_counts[w] + 1 if w in all_counts.keys() else 1\n",
    "    #Get the average of accumulated scores.\n",
    "    for w, s in all_scores.items():\n",
    "        all_scores[w] = s / all_counts[w]\n",
    "    words, finals = list(all_scores.keys()), list(all_scores.values())\n",
    "\n",
    "\n",
    "    #Sort the found substitutes by scores and print them out.\n",
    "    x = dict(zip(words, finals)) # list of words and final scores in dict form\n",
    "    sorted_list = list(sorted(x.items(), key=lambda item: item[1], reverse=True))[:topk] # take the \"topk\" best substitutes\n",
    "    #print([\"({0}: {1:0.8f})\".format(k, v) for k,v in sorted_list])\n",
    "    #print(\"Elapsed time: \", time.time() - start, \"\\n\")\n",
    "    return sorted_list"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "test = lexsub_dropout(\"Le chat est sur le tapis\", \"chat\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-14T10:01:33.164504Z",
     "start_time": "2024-05-14T10:01:32.189352Z"
    }
   },
   "id": "eed70ea457cb5a21",
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "id": "5ea09ae958f3609a",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# We apply this method to the dataset"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "model_name = \"Camembert\"\n",
    "all_predictions = {}"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-14T12:37:01.063124Z",
     "start_time": "2024-05-14T12:37:01.051494Z"
    }
   },
   "id": "d78ecb1b3acde6ff",
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running tour\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 14/582 [00:15<09:00,  1.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not find a substitution for   Au menu : du béton, des tours de verres géantes et de l'architecture moderne délirante. \n",
      "total of 1 skips for dataframe length 582\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 16/582 [00:16<07:51,  1.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not find a substitution for   Histoire de gagner du temps, on pourrait faire des synthèses : \" Le Roi du Maroc mange des côtelettes d'agneau préparées par des Kurdes au pied de la  Tour/ Eiffel \". \n",
      "total of 2 skips for dataframe length 582\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 24/582 [00:25<09:52,  1.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not find a substitution for   La cité était défendue par dix tours (voir le plan de la ville) et formait un polygone irrégulier ayant environ 4.400 mètres de développement (Voyez M. Leblanc, Recherches Historiques, t. 1, 45.) On connaà®t l'emplacement occupé par ces tours. \n",
      "total of 3 skips for dataframe length 582\n",
      "Could not find a substitution for   Trois tours s'élevaient sur le côté nord, on en remarque encore une très saillante dans un jardin. \n",
      "total of 4 skips for dataframe length 582\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 25/582 [00:26<09:17,  1.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not find a substitution for   La face ouest de la cité, qui était la plus exposée, était défendue par quatre tours , y compris celle d'Orbandelle, tandis que du côté du sud, qui était très escarpé, il n'y en avait qu'une seule, celle de Bruneau ou Brunehaut, appelée ainsi, à cause, dit-on, de la reine de ce nom qui l'avait fait restaurer. \n",
      "total of 5 skips for dataframe length 582\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 31/582 [00:30<08:07,  1.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not find a substitution for   Quelqu'un - mais qui ? - s'amuse, si l'on peut dire, à lui jouer de sales tours . \n",
      "total of 6 skips for dataframe length 582\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 34/582 [00:32<06:57,  1.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not find a substitution for  Tour d'horizon d'un look glamour furieusement tendance. \n",
      "total of 7 skips for dataframe length 582\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 40/582 [00:38<08:41,  1.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not find a substitution for   Dans l'entre-deux tours nous avons donc dialogué avec l'équipe d'Hélène Geoffroy et sommes arrivés à un accord sur un projet commun : priorité aux écoles, à l'emploi, à la sécurité et à une bonne gestion, le tout dans le cadre d'une bonne gouvernance. \n",
      "total of 8 skips for dataframe length 582\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 45/582 [00:42<08:21,  1.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not find a substitution for  Tour très simple à réaliser, avec un bon impact sur le public. \n",
      "total of 9 skips for dataframe length 582\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 53/582 [00:48<07:17,  1.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not find a substitution for   On passe au dessus de l'ancien refuge de la  Tour/ , en ruine. \n",
      "total of 10 skips for dataframe length 582\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 61/582 [00:59<10:40,  1.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not find a substitution for   Tout comme il y en a eu deux dans les tours jumelles. \n",
      "total of 11 skips for dataframe length 582\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 70/582 [01:08<07:34,  1.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not find a substitution for   Les deux tours emblématiques du World Trade Center à New-York - 2 Septembre 2001. \n",
      "total of 12 skips for dataframe length 582\n",
      "Could not find a substitution for   Ils vous emmèneront, entre autre, dans les tours de la fin du 12 ième siècle. \n",
      "total of 13 skips for dataframe length 582\n",
      "Could not find a substitution for   Une tyrolienne installée depuis la  Tour/ Eiffel pendant Roland-Garros \n",
      "total of 14 skips for dataframe length 582\n",
      "Could not find a substitution for   Jusqu'au 11 juin, une animation propose de se jeter du deuxième étage de la  Tour/ Eiffel. \n",
      "total of 15 skips for dataframe length 582\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 74/582 [01:08<03:16,  2.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not find a substitution for   L'expérience, appelée le \"Smash Perrier\", commence au deuxième étage de la  Tour/ Eiffel, à 115 mètres au dessus du sol. \n",
      "total of 16 skips for dataframe length 582\n",
      "Could not find a substitution for   Un dîner à la  Tour/ d'Argent. \n",
      "total of 17 skips for dataframe length 582\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 76/582 [01:11<05:34,  1.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not find a substitution for   En Occident, ce sont souvent des parcs d'amusements oà¹ les familles vont faire des tours de manèges. \n",
      "total of 18 skips for dataframe length 582\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▎        | 79/582 [01:12<05:17,  1.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not find a substitution for   Je n'ai plus le temps de regarder leurs émissions non plus et parfois ça me manque pas mal...La dernière que j'ai pris le temps d'en regarder une, Nino nous a fait de sacrés tours de magie *0*, trop fort...Je me demande si, comme moi, il a appris la magie grà¢ce à sa DS (cf : Master of Illusion ^__~) \n",
      "total of 19 skips for dataframe length 582\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 81/582 [01:14<05:12,  1.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not find a substitution for   A la fin de l'entretien on lui a demandé pourquoi les 2 tours de New York s'étaient effondrées. \n",
      "total of 20 skips for dataframe length 582\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 97/582 [01:27<04:53,  1.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not find a substitution for   Les organisateurs du  Tour/ des Flandres 2018 ont déniché une valeur sà»re pour doper l'audience. \n",
      "total of 21 skips for dataframe length 582\n",
      "Could not find a substitution for   Les gratte-ciels du globe ne se contentent plus seulement d'être hauts pour caresser les nuages, avec leurs architectures toujours plus ambitieuses et leurs façades écolo, ces tours innovantes ne passent plus inaperçues. \n",
      "total of 22 skips for dataframe length 582\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 582/582 [11:45<00:00,  1.21s/it]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'all_predictions' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[4], line 26\u001B[0m\n\u001B[0;32m     24\u001B[0m         skips \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[0;32m     25\u001B[0m         \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtotal of\u001B[39m\u001B[38;5;124m\"\u001B[39m, skips, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mskips for dataframe length\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28mlen\u001B[39m(df))\n\u001B[1;32m---> 26\u001B[0m \u001B[43mall_predictions\u001B[49m[keyword] \u001B[38;5;241m=\u001B[39m predictions\n",
      "\u001B[1;31mNameError\u001B[0m: name 'all_predictions' is not defined"
     ]
    }
   ],
   "source": [
    "lemmatizer = fll()\n",
    "for filename in os.listdir(f\"Corpus/Final/Automatic/\"):\n",
    "    if filename.endswith(\"Tour.csv\"):\n",
    "        df_gs = pd.read_csv(f\"Corpus/Final/Manual/\" + filename, sep=\";\", encoding=\"utf-8\", header=0)\n",
    "        df_automatic = pd.read_csv(f\"Corpus/Final/Automatic/\" + filename, sep=\";\", encoding=\"utf-8\", header=0)\n",
    "\n",
    "        df = pd.concat([df_gs, df_automatic], ignore_index=True)\n",
    "        # We calculate predictions\n",
    "        predictions = []\n",
    "        keyword = df[\"source\"][1].lower()\n",
    "        skips = 0\n",
    "        \n",
    "        print(\"running\", keyword)\n",
    "        sentences = df[\"match\"].values\n",
    "        if keyword == \"tour\":\n",
    "            sentences = [preproc_sentence(sentence) for sentence in sentences]\n",
    "        for sentence in tqdm(sentences):\n",
    "            # We do not apply the function if the literal keyword is not in the sentence\n",
    "            try:\n",
    "                substitutions = lexsub_dropout(sentence, keyword)\n",
    "                sentence_tuple = (sentence, substitutions)\n",
    "                predictions.append(sentence_tuple)\n",
    "            except:\n",
    "                print(\"Could not find a substitution for \", sentence)\n",
    "                skips += 1\n",
    "                print(\"total of\", skips, \"skips for dataframe length\", len(df))\n",
    "        all_predictions[keyword] = predictions\n",
    "        "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-14T12:36:45.288260Z",
     "start_time": "2024-05-14T12:24:54.222052Z"
    }
   },
   "id": "7347c22e42b370c3",
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# We save the predictions to a pandas dataframe (columns \"keyword\", 'sentence', 'predictions'\n",
    "if not os.path.exists(f\"{model_name}/Experiment_3/Automatic/predictions/\"):\n",
    "    os.makedirs(f\"{model_name}/Experiment_3/Automatic/predictions/\")\n",
    "for match, predictions in all_predictions.items():\n",
    "    df = pd.DataFrame(predictions, columns=[\"match\", \"predictions\"])\n",
    "    df[\"source\"] = match\n",
    "    # we add a 'match'\n",
    "    df.to_csv(f\"{model_name}/Experiment_3/Automatic/predictions/{match}_predictions.csv\", sep=\";\", encoding=\"utf-8\", index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-14T12:37:27.324345Z",
     "start_time": "2024-05-14T12:37:27.242096Z"
    }
   },
   "id": "4c43338b852fa9da",
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "df = pd.DataFrame(columns=[\"source\", \"match\", \"predictions\"])\n",
    "for match, predictions in all_predictions.items():\n",
    "    for sentence, subs in predictions:\n",
    "        new_row = pd.DataFrame({\"source\": [match], \"match\": [sentence], \"predictions\": [subs]})\n",
    "        df = pd.concat([df, new_row], ignore_index=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-14T12:37:28.537009Z",
     "start_time": "2024-05-14T12:37:28.062421Z"
    }
   },
   "id": "861192da02200917",
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# We then create a list of all the different words in \"forward\" and \"backward\"\n",
    "all_words = {}\n",
    "for match, predictions in all_predictions.items():\n",
    "    all_words[match] = []\n",
    "    for sentence, subs in predictions:\n",
    "        all_words[match].extend([prediction[0] for prediction in subs])\n",
    "    all_words[match] = list(set(all_words[match]))\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-14T12:37:28.922366Z",
     "start_time": "2024-05-14T12:37:28.912045Z"
    }
   },
   "id": "72b264034b69b1c5",
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "created dataframe for  tour\n",
      "converted tour into <class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "# Represent the sentences as sparse vectors\n",
    "# We do this by making a dataframe for each word: the index is the sentence, the columns are the words, and the values are the probabilities\n",
    "# We transform the dataframe\n",
    "vectors = {}\n",
    "for keyword, predictions in all_predictions.items():\n",
    "    df_sparse = pd.DataFrame(columns=[\"source\", \"match\"] + all_words[keyword])\n",
    "    print(\"created dataframe for \", keyword)\n",
    "    rows = []\n",
    "    for sentence, subs in predictions:\n",
    "        row = {\"source\": keyword, \"match\": sentence}\n",
    "        for word in all_words[keyword]:\n",
    "            row[word] = 0\n",
    "        for prediction in subs:\n",
    "            word, prob = prediction\n",
    "            row[word] = + prob  # We want to add to it if it already exists (forward and backward)\n",
    "        rows.append(row)\n",
    "    df_sparse = pd.DataFrame(rows)\n",
    "    vector = df_sparse.drop(columns=[\"source\", \"match\"]).values\n",
    "    # We now have a numpy ndarray, which we can transform using the TfidfTransformer\n",
    "    #transformer = TfidfTransformer()\n",
    "    finite_vals = vector[np.isfinite(vector)]\n",
    "    max_finite_val = np.max(finite_vals) if finite_vals.size > 0 else 1\n",
    "    vector[np.isinf(vector)] = max_finite_val\n",
    "    \n",
    "    # Replace NaNs with zero\n",
    "    vector[np.isnan(vector)] = 0\n",
    "    #vector_Tfidf = transformer.fit_transform(vector)\n",
    "    svd = TruncatedSVD(n_components=100)\n",
    "    vector_SVD = svd.fit_transform(vector)\n",
    "    vectors[keyword] = vector_SVD\n",
    "    print(f\"converted {keyword} into {type(vector)}\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-14T12:37:32.463689Z",
     "start_time": "2024-05-14T12:37:29.762630Z"
    }
   },
   "id": "f1843ab8a2d11e88",
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting model with 3 clusters\n",
      "Fitting model with 4 clusters\n",
      "Fitting model with 5 clusters\n",
      "Fitting model with 6 clusters\n",
      "Fitting model with 7 clusters\n",
      "Fitting model with 8 clusters\n",
      "Fitting model with 9 clusters\n",
      "Fitting model with 10 clusters\n",
      "Optimal number of clusters: 6\n",
      "agg:\n",
      "Score for each sense\n",
      "lap        0.000000\n",
      "round      0.000000\n",
      "tower      0.000000\n",
      "trick     14.285714\n",
      "turn     100.000000\n",
      "Name: agg_default_sub, dtype: float64\n",
      "Overall score 35.13513513513514\n",
      "BIC:\n",
      "Score for each sense\n",
      "lap      31.250000\n",
      "round    21.428571\n",
      "tower    16.666667\n",
      "trick    64.285714\n",
      "turn     29.166667\n",
      "Name: BIC_default_sub, dtype: float64\n",
      "Overall score 33.78378378378378\n"
     ]
    }
   ],
   "source": [
    "# Initialize the clustering algorithm\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "# 'n_clusters' is the number of clusters we want to form (and also the number of clusters to be found)\n",
    "# 'linkage' is the linkage criterion (can be 'ward', 'complete', 'average', 'single')\n",
    "clustermin = 3\n",
    "\n",
    "agg_clustering = AgglomerativeClustering(n_clusters=clustermin, metric='cosine', linkage='average')\n",
    "\n",
    "# Range of potential cluster numbers to test\n",
    "cluster_range = range(clustermin, 11)\n",
    "\n",
    "for keyword, vector_SVD in vectors.items():\n",
    "    df = pd.read_csv(f\"{model_name}/Experiment_1/Curated/{keyword}.csv\", sep=\";\", encoding=\"utf-8\", header=0)\n",
    "    df_gs = pd.read_csv(f\"Corpus/Final/Manual/{keyword}.csv\", sep=\";\", encoding=\"utf-8\", header=0)\n",
    "    df_automatic = pd.read_csv(f\"Corpus/Final/Automatic/{keyword}.csv\", sep=\";\", encoding=\"utf-8\", header=0)\n",
    "    df_together = pd.concat([df_gs, df_automatic], ignore_index=True)\n",
    "    # Apply the clustering algorithm to the SVD-transformed vectors\n",
    "    agg_clusters = agg_clustering.fit_predict(vector_SVD)\n",
    "\n",
    "    # 'agg_clusters' is now an array where the i-th element is the cluster label of the i-th instance\n",
    "    # We add \"1\" to all clusters to start counting at 1 instead of 0\n",
    "    agg_clusters += 1\n",
    "    # We add the cluster label to the original dataframe, corresponding to the correct index\n",
    "    agg_sentence_cluster_dict = dict(zip(df_together[\"match\"], agg_clusters))\n",
    "    df[\"agg_cluster_sub\"] = df[\"match\"].map(agg_sentence_cluster_dict)\n",
    "\n",
    "    # List to hold BIC values\n",
    "    bic_values = []\n",
    "\n",
    "    # Fit Gaussian Mixture Models for each number of clusters\n",
    "    for i in cluster_range:\n",
    "        print(f\"Fitting model with {i} clusters\")\n",
    "        gmm = GaussianMixture(n_components=i, random_state=0).fit(vector_SVD)\n",
    "        bic_values.append(gmm.bic(vector_SVD))\n",
    "\n",
    "    # Find the number of clusters that gives the minimum BIC\n",
    "    optimal_clusters = cluster_range[np.argmin(bic_values)]\n",
    "    print(f\"Optimal number of clusters: {optimal_clusters}\")\n",
    "\n",
    "    # Fit the optimal model\n",
    "    gmm_optimal = GaussianMixture(n_components=optimal_clusters).fit(vector_SVD)\n",
    "\n",
    "    # Predict the cluster for each data point\n",
    "    BIC_clusters = gmm_optimal.predict(vector_SVD)\n",
    "    # We want them to start counting at \"1\" instead of \"0\"\n",
    "    BIC_clusters += 1\n",
    "    # We now map these clusters to the original dataframe, for their respective \"sentence\"\n",
    "    BIC_sentence_cluster_dict = dict(zip(df_together[\"match\"], BIC_clusters))\n",
    "    # Map the cluster ids to the sentences in the dataframe\n",
    "    df['BIC_cluster_sub'] = df['match'].map(BIC_sentence_cluster_dict)\n",
    "\n",
    "    # We calculate a score for the clustering scores (starting before the outliers are removed)\n",
    "    # Group the dataframe by \"sense\" and \"cluster\", and calculate the size of each group\n",
    "    df_grouped = df.groupby([\"sense\", \"agg_cluster_sub\"]).size().reset_index(name=\"count\")\n",
    "\n",
    "    # Sort these clusters by size in descending order\n",
    "    df_grouped = df_grouped.sort_values(by=\"count\", ascending=False)\n",
    "\n",
    "    # Initialize an empty dictionary to store the cluster numbers that have been assigned as default clusters\n",
    "    # If the cluster number is not taken, we assign it to the corresponding \"sense\"\n",
    "    # Else, we try to assign it to the next cluster number\n",
    "    cluster_dict = {}\n",
    "    for index, row in df_grouped.iterrows():\n",
    "        if row[\"sense\"] not in cluster_dict:\n",
    "            if row[\"agg_cluster_sub\"] not in cluster_dict.values():\n",
    "                cluster_dict[row[\"sense\"]] = row[\"agg_cluster_sub\"]\n",
    "\n",
    "        # We add \"sense\" values that have no entry in cluster_dict and set value to 0 (always seen as wrong)\n",
    "    for sense in df[\"sense\"].unique():\n",
    "        if sense not in cluster_dict:\n",
    "            cluster_dict[sense] = 0\n",
    "\n",
    "    # Add a new column \"default\" to the original dataframe\n",
    "    df[\"agg_default_sub\"] = df.apply(lambda x: x[\"agg_cluster_sub\"] == cluster_dict[x[\"sense\"]], axis=1)\n",
    "\n",
    "    # We calculate the percentage of default clusters\n",
    "    percentage_default = (df[\"agg_default_sub\"].sum() / len(df)) * 100\n",
    "    # We also calculate this separately for each \"sense\"\n",
    "    percentage_default_mean = df.groupby(\"sense\")[\"agg_default_sub\"].mean() * 100\n",
    "\n",
    "    # We want the mean score across all senses, as it does not mean a lot if a program can correctly define one big cluster containing most of the data and fail at all other senses.\n",
    "    percentage_weighted = percentage_default_mean.mean()\n",
    "    print(\"agg:\")\n",
    "    print(\"Score for each\", percentage_default_mean)\n",
    "    print(\"Overall score\", percentage_default)\n",
    "\n",
    "    #----------------------------------------------------------------------------------------------------\n",
    "    # We do this again for BIC clusters\n",
    "    #----------------------------------------------------------------------------------------------------\n",
    "\n",
    "    # We calculate a score for the clustering scores (starting before the outliers are removed)\n",
    "    # Group the dataframe by \"sense\" and \"cluster\", and calculate the size of each group\n",
    "    df_grouped = df.groupby([\"sense\", \"BIC_cluster_sub\"]).size().reset_index(name=\"count\")\n",
    "\n",
    "    # Sort these clusters by size in descending order\n",
    "    df_grouped = df_grouped.sort_values(by=\"count\", ascending=False)\n",
    "\n",
    "    # Initialize an empty dictionary to store the cluster numbers that have been assigned as default clusters\n",
    "    # If the cluster number is not taken, we assign it to the corresponding \"sense\"\n",
    "    # Else, we try to assign it to the next cluster number\n",
    "    cluster_dict = {}\n",
    "    for index, row in df_grouped.iterrows():\n",
    "        if row[\"sense\"] not in cluster_dict:\n",
    "            if row[\"BIC_cluster_sub\"] not in cluster_dict.values():\n",
    "                cluster_dict[row[\"sense\"]] = row[\"BIC_cluster_sub\"]\n",
    "\n",
    "        # We add \"sense\" values that have no entry in cluster_dict and set value to 0 (always seen as wrong)\n",
    "    for sense in df[\"sense\"].unique():\n",
    "        if sense not in cluster_dict:\n",
    "            cluster_dict[sense] = 0\n",
    "\n",
    "    # Add a new column \"default\" to the original dataframe\n",
    "    df[\"BIC_default_sub\"] = df.apply(lambda x: x[\"BIC_cluster_sub\"] == cluster_dict[x[\"sense\"]], axis=1)\n",
    "\n",
    "    # We calculate the percentage of default clusters\n",
    "    percentage_default = (df[\"BIC_default_sub\"].sum() / len(df)) * 100\n",
    "    # We also calculate this separately for each \"sense\"\n",
    "    percentage_default_mean = df.groupby(\"sense\")[\"BIC_default_sub\"].mean() * 100\n",
    "\n",
    "    # We want the mean score across all senses, as it does not mean a lot if a program can correctly define one big cluster containing most of the data and fail at all other senses.\n",
    "    percentage_weighted = percentage_default_mean.mean()\n",
    "    print(\"BIC:\")\n",
    "    print(\"Score for each\", percentage_default_mean)\n",
    "    print(\"Overall score\", percentage_default)\n",
    "\n",
    "    df.to_csv(f\"{model_name}/Experiment_3/Automatic/{keyword}.csv\", sep=\";\", encoding=\"utf-8\", index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-14T12:38:06.790082Z",
     "start_time": "2024-05-14T12:38:06.049364Z"
    }
   },
   "id": "7c8bbf9e123673b3",
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-14T12:38:00.490350Z",
     "start_time": "2024-05-14T12:38:00.486456Z"
    }
   },
   "id": "f7f648af6d449879",
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "fcbedcaf3c71f308"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
