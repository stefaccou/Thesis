{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-05-17T20:57:56.010540Z",
     "start_time": "2024-05-17T20:57:56.005118Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, CamembertForMaskedLM\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import plotly.express as px\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.mixture import GaussianMixture\n",
    "import numpy as np\n",
    "import warnings\n",
    "import torch.nn.functional as F\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "import ast\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from french_lefff_lemmatizer.french_lefff_lemmatizer import FrenchLefffLemmatizer as fll\n",
    "import nltk\n",
    "#nltk.download(\"stopwords\")\n",
    "import string\n"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at camembert-base were not used when initializing CamembertForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing CamembertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing CamembertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"camembert-base\")\n",
    "model = CamembertForMaskedLM.from_pretrained(\"camembert-base\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-17T20:57:58.517729Z",
     "start_time": "2024-05-17T20:57:56.916028Z"
    }
   },
   "id": "8146f013d080ab3f",
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "CamembertForMaskedLM(\n  (roberta): CamembertModel(\n    (embeddings): CamembertEmbeddings(\n      (word_embeddings): Embedding(32005, 768, padding_idx=1)\n      (position_embeddings): Embedding(514, 768, padding_idx=1)\n      (token_type_embeddings): Embedding(1, 768)\n      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (encoder): CamembertEncoder(\n      (layer): ModuleList(\n        (0-11): 12 x CamembertLayer(\n          (attention): CamembertAttention(\n            (self): CamembertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): CamembertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): CamembertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): CamembertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n  )\n  (lm_head): CamembertLMHead(\n    (dense): Linear(in_features=768, out_features=768, bias=True)\n    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n    (decoder): Linear(in_features=768, out_features=32005, bias=True)\n  )\n)"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cuda = torch.device('cuda')\n",
    "model.to(cuda)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-17T20:57:58.773979Z",
     "start_time": "2024-05-17T20:57:58.519726Z"
    }
   },
   "id": "c76fa8df8b198343",
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "model_name = \"Camembert\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-17T20:57:58.779749Z",
     "start_time": "2024-05-17T20:57:58.775979Z"
    }
   },
   "id": "6aaf20a0ce417a82",
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def filter_substitutions(substitutions): #adapted filter function from https://github.com/jvladika/Lexical-Substitution.git\n",
    "    dels = list()\n",
    "    for sub in substitutions:\n",
    "        if sub.lower() in substitutions and sub.capitalize() in substitutions:\n",
    "            dels.append(sub.capitalize())\n",
    "        if sub.lower() in substitutions and sub.upper() in substitutions:\n",
    "            dels.append(sub.upper())\n",
    "        if sub in nltk.corpus.stopwords.words('french') or sub in string.punctuation:\n",
    "            dels.append(sub)\n",
    "    dels = list(set(dels))\n",
    "    for d in dels:\n",
    "        substitutions.remove(d)\n",
    "    return substitutions\n",
    "def get_token_predictions(substituted_sentence, keyword, top_k=3):\n",
    "    blacklist = [keyword, keyword.capitalize(), keyword.lower(), \n",
    "                 \"</s>\", \"\", \"-\", '\"', \"'\", \",\", \"le\", \"la\", \"l'\", \"l\", \"les\", \"un\", \"une\", \"des\",\n",
    "                \"et\", \"/\", \".\", \"(\", \")\", \"du\", \"de\", \"d'\", \"ce\", \"cet\", \"cette\", \"ces\", \"qui\", \"que\", \n",
    "                 \"à\", \"au\", \"aux\", \"en\", \"dans\", \"par\", \"pour\", \"sur\", \"avec\", \"sans\", \"sous\", \"entre\",\n",
    "                 \"...\", \":\", \";\", \"!\", \"?\", \"«\", \"»\", \"(\", \")\", \"[\", \"]\", \"{\", \"}\", \"–\", \"—\", \"–\", \"—\", \"‘\", \"’\", \"“\", \"+\"\n",
    "                 ]\n",
    "    inputs = tokenizer(substituted_sentence, return_tensors=\"pt\")\n",
    "    inputs = inputs.to(cuda)\n",
    "    with torch.no_grad():\n",
    "        logits = model(**inputs).logits\n",
    "    mask_token_index = (inputs.input_ids == tokenizer.mask_token_id)[0].nonzero(as_tuple=True)[0]\n",
    "    init_k = 10\n",
    "    while True:\n",
    "        # Get the top k predicted token ids\n",
    "        predicted_token_ids = logits[0, mask_token_index].topk(init_k).indices\n",
    "        predicted_token_probs = F.softmax(logits[0, mask_token_index].topk(init_k).values, dim=-1).cpu().numpy()\n",
    "\n",
    "        # Convert the token ids to tokens\n",
    "        predicted_token_ids = predicted_token_ids.flatten().tolist()\n",
    "        predicted_token_probs = predicted_token_probs.flatten().tolist()\n",
    "\n",
    "        substitutions = tokenizer.convert_ids_to_tokens(predicted_token_ids)\n",
    "\n",
    "        substitutions = [substitution.replace('▁', '') for substitution in substitutions]\n",
    "        substitutions = filter_substitutions(substitutions)\n",
    "        substitutions_with_probs = [(substitution, prob) for substitution, prob in\n",
    "                                    zip(substitutions, predicted_token_probs) if substitution not in blacklist]\n",
    "\n",
    "        if len(substitutions) >= top_k:\n",
    "            break\n",
    "\n",
    "        init_k += 5\n",
    "\n",
    "    substitutions_with_probs = substitutions_with_probs[:top_k]\n",
    "    return substitutions_with_probs\n",
    "\n",
    "\n",
    "def return_masked_predictions_probs(sentence, keyword, top_k=3):\n",
    "    zinput_forward = sentence.replace(keyword, f\"<mask> et {keyword}\", 1)\n",
    "    zinput_backward = sentence.replace(keyword, f\"{keyword} et <mask>\", 1)\n",
    "    if zinput_forward == sentence or zinput_backward == sentence:\n",
    "        return [\"Keyword not found\"] * 2\n",
    "\n",
    "    # forward:\n",
    "    substitutions_forward = get_token_predictions(zinput_forward, keyword, top_k=top_k)\n",
    "    # backward:\n",
    "    substitutions_backward = get_token_predictions(zinput_backward, keyword, top_k=top_k)\n",
    "    return substitutions_forward, substitutions_backward\n",
    "\n",
    "WSD_PATTERN = r' (\\w+)/\\w+\\.\\w\\.\\d+' # (Woord), letterlijke slash, woord, letterlijke punt, letter, cijfer(s)\n",
    "\n",
    "def preproc_sentence(sentence):\n",
    "    sent_preproc = re.sub(WSD_PATTERN, r'\\1', sentence) # Alleen woord blijft over\n",
    "    return sent_preproc"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-17T20:57:59.437711Z",
     "start_time": "2024-05-17T20:57:59.425758Z"
    }
   },
   "id": "fd9a7b0b319966fa",
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Getting the substitute vectors\n",
    "\n",
    "We will also use the automatic download dataset from experiment 2"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2384ad69e28b403a"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running avocat\n",
      "running bien\n",
      "running bureau\n",
      "running faculté\n",
      "running filer\n",
      "running glace\n",
      "running souris\n",
      "running supporter\n",
      "running tirer\n",
      "running tour\n",
      "running vol\n"
     ]
    }
   ],
   "source": [
    "all_predictions = {}\n",
    "lemmatizer = fll()\n",
    "for filename in os.listdir(f\"Corpus/Final/Automatic/\"):\n",
    "    if filename.endswith(\".csv\"):\n",
    "        df_gs = pd.read_csv(f\"Corpus/Final/Manual/\" + filename, sep=\";\", encoding=\"utf-8\", header=0)\n",
    "        df_automatic = pd.read_csv(f\"Corpus/Final/Automatic/\" + filename, sep=\";\", encoding=\"utf-8\", header=0)\n",
    "        df = pd.concat([df_gs, df_automatic], ignore_index=True)\n",
    "        # We calculate predictions\n",
    "        predictions = []\n",
    "        keyword = df[\"source\"][1].lower()\n",
    "        print(\"running\", keyword)\n",
    "        sentences = df[\"match\"].values\n",
    "        if keyword == \"tour\":\n",
    "            sentences = [preproc_sentence(sentence) for sentence in sentences]\n",
    "        for sentence in sentences:\n",
    "            forward, backward = return_masked_predictions_probs(sentence, keyword, top_k=10)\n",
    "            if \"Keyword not found\" in forward or \"Keyword not found\" in backward:\n",
    "                continue\n",
    "            forward = [(lemmatizer.lemmatize(prediction[0]), prediction[1]) for prediction in forward]\n",
    "            backward = [(lemmatizer.lemmatize(prediction[0]), prediction[1]) for prediction in backward]\n",
    "            #print(f\"Forward: {forward}\\tBackward: {backward}\")\n",
    "            sentence_tuple = (sentence, forward, backward)\n",
    "            predictions.append(sentence_tuple)\n",
    "        all_predictions[keyword] = predictions\n",
    "        "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-17T21:03:27.091702Z",
     "start_time": "2024-05-17T20:58:00.818838Z"
    }
   },
   "id": "649e31ca22b6cd42",
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# We save this to a dataframe\n",
    "df = pd.DataFrame(columns=[\"source\", \"match\", \"predictions\"])\n",
    "for match, predictions in all_predictions.items():\n",
    "    for sentence, forward, backward in predictions:\n",
    "        new_row = pd.DataFrame({\"source\": [match], \"match\": [sentence], \"predictions\": [(forward, backward)]})\n",
    "        df = pd.concat([df, new_row], ignore_index=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-17T21:03:29.477445Z",
     "start_time": "2024-05-17T21:03:27.092737Z"
    }
   },
   "id": "f5e724391d300beb",
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "if not os.path.exists(f\"{model_name}/Experiment_2/Automatic/predictions/together\"):\n",
    "    os.makedirs(f\"{model_name}/Experiment_2/Automatic/predictions/together\")\n",
    "df.to_csv(f\"{model_name}/Experiment_2/Automatic/predictions/together/predictions.csv\", sep=\";\", encoding=\"utf-8\", index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-17T21:03:29.832960Z",
     "start_time": "2024-05-17T21:03:29.478483Z"
    }
   },
   "id": "8865d6038930ed18",
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# We also save these to different files, one for each different \"source\"\n",
    "for match, predictions in all_predictions.items():\n",
    "    df_match = pd.DataFrame(columns=[\"source\", \"match\", \"predictions\"])\n",
    "    for sentence, forward, backward in predictions:\n",
    "        new_row = pd.DataFrame({\"source\": [match], \"match\": [sentence], \"predictions\": [(forward, backward)]})\n",
    "        df_match = pd.concat([df, new_row], ignore_index=True)\n",
    "    df_match.to_csv(f\"{model_name}/Experiment_2/Automatic/predictions/{match}.csv\", sep=\";\", encoding=\"utf-8\", index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-17T21:03:36.171290Z",
     "start_time": "2024-05-17T21:03:29.834957Z"
    }
   },
   "id": "9d134c2b81fc3c0c",
   "execution_count": 12
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Creating sparse vectors based on the predictions\n",
    "For each source word, we create sparse vectors representing each match:\n",
    "We create a vector of the length of the amount of items for each word,\n",
    "then we add the prediction probability for the corresponding word to the vector."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "58c96e1185b81535"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# We then create a list of all the different words in \"forward\" and \"backward\"\n",
    "all_words = {}\n",
    "for match, predictions in all_predictions.items():\n",
    "    all_words[match] = []\n",
    "    for sentence, forward, backward in predictions:\n",
    "        all_words[match].extend([prediction[0] for prediction in forward + backward])\n",
    "    all_words[match] = list(set(all_words[match]))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-17T21:03:36.195715Z",
     "start_time": "2024-05-17T21:03:36.172335Z"
    }
   },
   "id": "f6fe92b7cdba21a6",
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "created dataframe for  avocat\n",
      "converted avocat into <class 'numpy.ndarray'>\n",
      "created dataframe for  bien\n",
      "converted bien into <class 'numpy.ndarray'>\n",
      "created dataframe for  bureau\n",
      "converted bureau into <class 'numpy.ndarray'>\n",
      "created dataframe for  faculté\n",
      "converted faculté into <class 'numpy.ndarray'>\n",
      "created dataframe for  filer\n",
      "converted filer into <class 'numpy.ndarray'>\n",
      "created dataframe for  glace\n",
      "converted glace into <class 'numpy.ndarray'>\n",
      "created dataframe for  souris\n",
      "converted souris into <class 'numpy.ndarray'>\n",
      "created dataframe for  supporter\n",
      "converted supporter into <class 'numpy.ndarray'>\n",
      "created dataframe for  tirer\n",
      "converted tirer into <class 'numpy.ndarray'>\n",
      "created dataframe for  tour\n",
      "converted tour into <class 'numpy.ndarray'>\n",
      "created dataframe for  vol\n",
      "converted vol into <class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "# Represent the sentences as sparse vectors\n",
    "# We do this by making a dataframe for each word: the index is the sentence, the columns are the words, and the values are the probabilities\n",
    "# We transform the dataframe\n",
    "vectors = {}\n",
    "for keyword, predictions in all_predictions.items():\n",
    "    df_sparse = pd.DataFrame(columns=[\"source\", \"match\"] + all_words[keyword])\n",
    "    print(\"created dataframe for \", keyword)\n",
    "    rows = []\n",
    "    for sentence, forward, backward in predictions:\n",
    "            row = {\"source\": keyword, \"match\": sentence}\n",
    "            for word in all_words[keyword]:\n",
    "                row[word] = 0\n",
    "            for prediction in forward + backward:\n",
    "                word, prob = prediction\n",
    "                row[word] =+ prob # We want to add to it if it already exists (forward and backward)\n",
    "            rows.append(row)\n",
    "    df_sparse = pd.DataFrame(rows)\n",
    "    vector = df_sparse.drop(columns=[\"source\", \"match\"]).values\n",
    "    # We now have a numpy ndarray, which we can transform using the TfidfTransformer\n",
    "    #transformer = TfidfTransformer()\n",
    "    #vector_Tfidf = transformer.fit_transform(vector)\n",
    "    svd = TruncatedSVD(n_components=100)\n",
    "    vector_SVD = svd.fit_transform(vector)\n",
    "    vectors[keyword] = vector_SVD\n",
    "    print(f\"converted {keyword} into {type(vector)}\")\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-17T21:03:49.196581Z",
     "start_time": "2024-05-17T21:03:36.197715Z"
    }
   },
   "id": "3c8d45bed64f82b0",
   "execution_count": 14
  },
  {
   "cell_type": "markdown",
   "source": [
    "Amrami and Goldberg state that using the TF-IDF transformation can help to improve the results of the clustering. We will apply this transformation to the sparse vectors."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "16529e5f61a02770"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting model with 3 clusters\n",
      "Fitting model with 4 clusters\n",
      "Fitting model with 5 clusters\n",
      "Fitting model with 6 clusters\n",
      "Fitting model with 7 clusters\n",
      "Fitting model with 8 clusters\n",
      "Fitting model with 9 clusters\n",
      "Fitting model with 10 clusters\n",
      "Optimal number of clusters: 3\n",
      "Number of clusters: 3\n",
      "agg:\n",
      "Score for each sense\n",
      "avocado    82.142857\n",
      "lawyer     96.428571\n",
      "Name: agg_default_sub, dtype: float64\n",
      "Overall score 91.66666666666666\n",
      "BIC:\n",
      "Score for each sense\n",
      "avocado    96.428571\n",
      "lawyer     32.142857\n",
      "Name: BIC_default_sub, dtype: float64\n",
      "Overall score 53.57142857142857\n",
      "Fitting model with 3 clusters\n",
      "Fitting model with 4 clusters\n",
      "Fitting model with 5 clusters\n",
      "Fitting model with 6 clusters\n",
      "Fitting model with 7 clusters\n",
      "Fitting model with 8 clusters\n",
      "Fitting model with 9 clusters\n",
      "Fitting model with 10 clusters\n",
      "Optimal number of clusters: 3\n",
      "Number of clusters: 3\n",
      "agg:\n",
      "Score for each sense\n",
      "good           0.0\n",
      "property     100.0\n",
      "wellbeing      0.0\n",
      "Name: agg_default_sub, dtype: float64\n",
      "Overall score 50.0\n",
      "BIC:\n",
      "Score for each sense\n",
      "good         55.172414\n",
      "property     42.857143\n",
      "wellbeing    20.000000\n",
      "Name: BIC_default_sub, dtype: float64\n",
      "Overall score 41.83673469387755\n",
      "Fitting model with 3 clusters\n",
      "Fitting model with 4 clusters\n",
      "Fitting model with 5 clusters\n",
      "Fitting model with 6 clusters\n",
      "Fitting model with 7 clusters\n",
      "Fitting model with 8 clusters\n",
      "Fitting model with 9 clusters\n",
      "Fitting model with 10 clusters\n",
      "Optimal number of clusters: 3\n",
      "Number of clusters: 3\n",
      "agg:\n",
      "Score for each sense\n",
      "cabinet      0.0\n",
      "desk         0.0\n",
      "office     100.0\n",
      "Name: agg_default_sub, dtype: float64\n",
      "Overall score 52.42718446601942\n",
      "BIC:\n",
      "Score for each sense\n",
      "cabinet     0.000000\n",
      "desk        2.857143\n",
      "office     96.296296\n",
      "Name: BIC_default_sub, dtype: float64\n",
      "Overall score 51.45631067961165\n",
      "Fitting model with 3 clusters\n",
      "Fitting model with 4 clusters\n",
      "Fitting model with 5 clusters\n",
      "Fitting model with 6 clusters\n",
      "Fitting model with 7 clusters\n",
      "Fitting model with 8 clusters\n",
      "Fitting model with 9 clusters\n",
      "Fitting model with 10 clusters\n",
      "Optimal number of clusters: 4\n",
      "Number of clusters: 4\n",
      "agg:\n",
      "Score for each sense\n",
      "ability        0.000000\n",
      "competence    85.714286\n",
      "university    43.243243\n",
      "Name: agg_default_sub, dtype: float64\n",
      "Overall score 50.0\n",
      "BIC:\n",
      "Score for each sense\n",
      "ability        0.000000\n",
      "competence    42.857143\n",
      "university    83.783784\n",
      "Name: BIC_default_sub, dtype: float64\n",
      "Overall score 53.75\n",
      "Fitting model with 3 clusters\n",
      "Fitting model with 4 clusters\n",
      "Fitting model with 5 clusters\n",
      "Fitting model with 6 clusters\n",
      "Fitting model with 7 clusters\n",
      "Fitting model with 8 clusters\n",
      "Fitting model with 9 clusters\n",
      "Fitting model with 10 clusters\n",
      "Optimal number of clusters: 4\n",
      "Number of clusters: 4\n",
      "agg:\n",
      "Score for each sense\n",
      "give           3.448276\n",
      "slip away    100.000000\n",
      "spinning       0.000000\n",
      "Name: agg_default_sub, dtype: float64\n",
      "Overall score 50.0\n",
      "BIC:\n",
      "Score for each sense\n",
      "give         100.000000\n",
      "slip away     68.085106\n",
      "spinning       0.000000\n",
      "Name: BIC_default_sub, dtype: float64\n",
      "Overall score 63.541666666666664\n",
      "Fitting model with 3 clusters\n",
      "Fitting model with 4 clusters\n",
      "Fitting model with 5 clusters\n",
      "Fitting model with 6 clusters\n",
      "Fitting model with 7 clusters\n",
      "Fitting model with 8 clusters\n",
      "Fitting model with 9 clusters\n",
      "Fitting model with 10 clusters\n",
      "Optimal number of clusters: 3\n",
      "Number of clusters: 3\n",
      "agg:\n",
      "Score for each sense\n",
      "glass              0.000000\n",
      "ice               83.050847\n",
      "ice cream         65.000000\n",
      "mirror            11.538462\n",
      "powdered sugar     0.000000\n",
      "Name: agg_default_sub, dtype: float64\n",
      "Overall score 52.84552845528455\n",
      "BIC:\n",
      "Score for each sense\n",
      "glass              0.000000\n",
      "ice               93.220339\n",
      "ice cream         45.000000\n",
      "mirror             7.692308\n",
      "powdered sugar     0.000000\n",
      "Name: BIC_default_sub, dtype: float64\n",
      "Overall score 53.65853658536586\n",
      "Fitting model with 3 clusters\n",
      "Fitting model with 4 clusters\n",
      "Fitting model with 5 clusters\n",
      "Fitting model with 6 clusters\n",
      "Fitting model with 7 clusters\n",
      "Fitting model with 8 clusters\n",
      "Fitting model with 9 clusters\n",
      "Fitting model with 10 clusters\n",
      "Optimal number of clusters: 3\n",
      "Number of clusters: 3\n",
      "agg:\n",
      "Score for each sense\n",
      "bat          0.0\n",
      "pointer      0.0\n",
      "rodent     100.0\n",
      "Name: agg_default_sub, dtype: float64\n",
      "Overall score 45.588235294117645\n",
      "BIC:\n",
      "Score for each sense\n",
      "bat         0.000000\n",
      "pointer    66.666667\n",
      "rodent     67.741935\n",
      "Name: BIC_default_sub, dtype: float64\n",
      "Overall score 57.35294117647059\n",
      "Fitting model with 3 clusters\n",
      "Fitting model with 4 clusters\n",
      "Fitting model with 5 clusters\n",
      "Fitting model with 6 clusters\n",
      "Fitting model with 7 clusters\n",
      "Fitting model with 8 clusters\n",
      "Fitting model with 9 clusters\n",
      "Fitting model with 10 clusters\n",
      "Optimal number of clusters: 3\n",
      "Number of clusters: 3\n",
      "agg:\n",
      "Score for each sense\n",
      "endure     100.0\n",
      "fan        100.0\n",
      "hold up      0.0\n",
      "Name: agg_default_sub, dtype: float64\n",
      "Overall score 72.72727272727273\n",
      "BIC:\n",
      "Score for each sense\n",
      "endure      89.285714\n",
      "fan        100.000000\n",
      "hold up      0.000000\n",
      "Name: BIC_default_sub, dtype: float64\n",
      "Overall score 67.27272727272727\n",
      "Fitting model with 3 clusters\n",
      "Fitting model with 4 clusters\n",
      "Fitting model with 5 clusters\n",
      "Fitting model with 6 clusters\n",
      "Fitting model with 7 clusters\n",
      "Fitting model with 8 clusters\n",
      "Fitting model with 9 clusters\n",
      "Fitting model with 10 clusters\n",
      "Optimal number of clusters: 3\n",
      "Number of clusters: 3\n",
      "agg:\n",
      "Score for each sense\n",
      "pull      6.666667\n",
      "shoot    95.945946\n",
      "Name: agg_default_sub, dtype: float64\n",
      "Overall score 70.1923076923077\n",
      "BIC:\n",
      "Score for each sense\n",
      "pull      3.333333\n",
      "shoot    97.297297\n",
      "Name: BIC_default_sub, dtype: float64\n",
      "Overall score 70.1923076923077\n",
      "Fitting model with 3 clusters\n",
      "Fitting model with 4 clusters\n",
      "Fitting model with 5 clusters\n",
      "Fitting model with 6 clusters\n",
      "Fitting model with 7 clusters\n",
      "Fitting model with 8 clusters\n",
      "Fitting model with 9 clusters\n",
      "Fitting model with 10 clusters\n",
      "Optimal number of clusters: 3\n",
      "Number of clusters: 3\n",
      "agg:\n",
      "Score for each sense\n",
      "lap        0.0\n",
      "round      0.0\n",
      "tower      0.0\n",
      "trick      0.0\n",
      "turn     100.0\n",
      "Name: agg_default_sub, dtype: float64\n",
      "Overall score 32.432432432432435\n",
      "BIC:\n",
      "Score for each sense\n",
      "lap      12.500000\n",
      "round     0.000000\n",
      "tower     0.000000\n",
      "trick     0.000000\n",
      "turn     79.166667\n",
      "Name: BIC_default_sub, dtype: float64\n",
      "Overall score 28.37837837837838\n",
      "Fitting model with 3 clusters\n",
      "Fitting model with 4 clusters\n",
      "Fitting model with 5 clusters\n",
      "Fitting model with 6 clusters\n",
      "Fitting model with 7 clusters\n",
      "Fitting model with 8 clusters\n",
      "Fitting model with 9 clusters\n",
      "Fitting model with 10 clusters\n",
      "Optimal number of clusters: 3\n",
      "Number of clusters: 3\n",
      "agg:\n",
      "Score for each sense\n",
      "flight    100.0\n",
      "theft       0.0\n",
      "Name: agg_default_sub, dtype: float64\n",
      "Overall score 70.58823529411765\n",
      "BIC:\n",
      "Score for each sense\n",
      "flight    70.0\n",
      "theft     20.0\n",
      "Name: BIC_default_sub, dtype: float64\n",
      "Overall score 55.294117647058826\n"
     ]
    }
   ],
   "source": [
    "# Initialize the clustering algorithm\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "# 'n_clusters' is the number of clusters we want to form (and also the number of clusters to be found)\n",
    "# 'linkage' is the linkage criterion (can be 'ward', 'complete', 'average', 'single')\n",
    "clustermin = 3\n",
    "\n",
    "agg_clustering = AgglomerativeClustering(n_clusters=clustermin, metric='cosine', linkage='average')\n",
    "\n",
    "# Range of potential cluster numbers to test\n",
    "cluster_range = range(clustermin,11)\n",
    "\n",
    "\n",
    "for keyword, vector_SVD in vectors.items():\n",
    "    df = pd.read_csv(f\"{model_name}/Experiment_1/Curated/{keyword}.csv\", sep=\";\", encoding=\"utf-8\", header=0)\n",
    "    df_gs = pd.read_csv(f\"Corpus/Final/Manual/{keyword}.csv\", sep=\";\", encoding=\"utf-8\", header=0)\n",
    "    df_automatic = pd.read_csv(f\"Corpus/Final/Automatic/{keyword}.csv\", sep=\";\", encoding=\"utf-8\", header=0)\n",
    "    df_together = pd.concat([df_gs, df_automatic], ignore_index=True)\n",
    "    # Apply the clustering algorithm to the SVD-transformed vectors\n",
    "    agg_clusters = agg_clustering.fit_predict(vector_SVD)\n",
    "\n",
    "    # 'agg_clusters' is now an array where the i-th element is the cluster label of the i-th instance\n",
    "    # We add \"1\" to all clusters to start counting at 1 instead of 0\n",
    "    agg_clusters += 1\n",
    "    # We add the cluster label to the original dataframe, corresponding to the correct index\n",
    "    agg_sentence_cluster_dict = dict(zip(df_together[\"match\"], agg_clusters))\n",
    "    df[\"agg_cluster_sub\"] = df[\"match\"].map(agg_sentence_cluster_dict)\n",
    "    \n",
    "    \n",
    "    # List to hold BIC values\n",
    "    bic_values = []\n",
    "\n",
    "    \n",
    "    # Fit Gaussian Mixture Models for each number of clusters\n",
    "    for i in cluster_range:\n",
    "        print(f\"Fitting model with {i} clusters\")\n",
    "        gmm = GaussianMixture(n_components=i, random_state=0).fit(vector_SVD)\n",
    "        bic_values.append(gmm.bic(vector_SVD))\n",
    "    \n",
    "    # Find the number of clusters that gives the minimum BIC\n",
    "    optimal_clusters = cluster_range[np.argmin(bic_values)]\n",
    "    print(f\"Optimal number of clusters: {optimal_clusters}\")\n",
    "    \n",
    "    # Fit the optimal model\n",
    "    gmm_optimal = GaussianMixture(n_components=optimal_clusters).fit(vector_SVD)\n",
    "    \n",
    "    # Predict the cluster for each data point\n",
    "    BIC_clusters = gmm_optimal.predict(vector_SVD)\n",
    "    # We want them to start counting at \"1\" instead of \"0\"\n",
    "    BIC_clusters += 1\n",
    "    print(f\"Number of clusters: {len(np.unique(BIC_clusters))}\")\n",
    "    # We now map these clusters to the original dataframe, for their respective \"sentence\"\n",
    "    BIC_sentence_cluster_dict = dict(zip(df_together[\"match\"], BIC_clusters))\n",
    "    # Map the cluster ids to the sentences in the dataframe\n",
    "    df['BIC_cluster_sub'] = df['match'].map(BIC_sentence_cluster_dict)\n",
    "        \n",
    "\n",
    "    # We calculate a score for the clustering scores (starting before the outliers are removed)\n",
    "    # Group the dataframe by \"sense\" and \"cluster\", and calculate the size of each group\n",
    "    df_grouped = df.groupby([\"sense\", \"agg_cluster_sub\"]).size().reset_index(name=\"count\")\n",
    "\n",
    "    # Sort these clusters by size in descending order\n",
    "    df_grouped = df_grouped.sort_values(by=\"count\", ascending=False)\n",
    "\n",
    "    # Initialize an empty dictionary to store the cluster numbers that have been assigned as default clusters\n",
    "    # If the cluster number is not taken, we assign it to the corresponding \"sense\"\n",
    "    # Else, we try to assign it to the next cluster number\n",
    "    cluster_dict = {}\n",
    "    for index, row in df_grouped.iterrows():\n",
    "        if row[\"sense\"] not in cluster_dict:\n",
    "            if row[\"agg_cluster_sub\"] not in cluster_dict.values():\n",
    "                cluster_dict[row[\"sense\"]] = row[\"agg_cluster_sub\"]\n",
    "\n",
    "        # We add \"sense\" values that have no entry in cluster_dict and set value to 0 (always seen as wrong)\n",
    "    for sense in df[\"sense\"].unique():\n",
    "        if sense not in cluster_dict:\n",
    "            cluster_dict[sense] = 0\n",
    "\n",
    "    # Add a new column \"default\" to the original dataframe\n",
    "    df[\"agg_default_sub\"] = df.apply(lambda x: x[\"agg_cluster_sub\"] == cluster_dict[x[\"sense\"]], axis=1)\n",
    "    \n",
    "    # We calculate the percentage of default clusters\n",
    "    percentage_default = (df[\"agg_default_sub\"].sum() / len(df)) * 100\n",
    "    # We also calculate this separately for each \"sense\"\n",
    "    percentage_default_mean = df.groupby(\"sense\")[\"agg_default_sub\"].mean() * 100\n",
    "\n",
    "    # We want the mean score across all senses, as it does not mean a lot if a program can correctly define one big cluster containing most of the data and fail at all other senses.\n",
    "    percentage_weighted = percentage_default_mean.mean()\n",
    "    print(\"agg:\")\n",
    "    print(\"Score for each\", percentage_default_mean)\n",
    "    print(\"Overall score\", percentage_default)   \n",
    "    \n",
    "    #----------------------------------------------------------------------------------------------------\n",
    "    # We do this again for BIC clusters\n",
    "    #----------------------------------------------------------------------------------------------------\n",
    " \n",
    "    # We calculate a score for the clustering scores (starting before the outliers are removed)\n",
    "    # Group the dataframe by \"sense\" and \"cluster\", and calculate the size of each group\n",
    "    df_grouped = df.groupby([\"sense\", \"BIC_cluster_sub\"]).size().reset_index(name=\"count\")\n",
    "\n",
    "    # Sort these clusters by size in descending order\n",
    "    df_grouped = df_grouped.sort_values(by=\"count\", ascending=False)\n",
    "\n",
    "    # Initialize an empty dictionary to store the cluster numbers that have been assigned as default clusters\n",
    "    # If the cluster number is not taken, we assign it to the corresponding \"sense\"\n",
    "    # Else, we try to assign it to the next cluster number\n",
    "    cluster_dict = {}\n",
    "    for index, row in df_grouped.iterrows():\n",
    "        if row[\"sense\"] not in cluster_dict:\n",
    "            if row[\"BIC_cluster_sub\"] not in cluster_dict.values():\n",
    "                cluster_dict[row[\"sense\"]] = row[\"BIC_cluster_sub\"]\n",
    "\n",
    "        # We add \"sense\" values that have no entry in cluster_dict and set value to 0 (always seen as wrong)\n",
    "    for sense in df[\"sense\"].unique():\n",
    "        if sense not in cluster_dict:\n",
    "            cluster_dict[sense] = 0\n",
    "\n",
    "    # Add a new column \"default\" to the original dataframe\n",
    "    df[\"BIC_default_sub\"] = df.apply(lambda x: x[\"BIC_cluster_sub\"] == cluster_dict[x[\"sense\"]], axis=1)\n",
    "    \n",
    "    # We calculate the percentage of default clusters\n",
    "    percentage_default = (df[\"BIC_default_sub\"].sum() / len(df)) * 100\n",
    "    # We also calculate this separately for each \"sense\"\n",
    "    percentage_default_mean = df.groupby(\"sense\")[\"BIC_default_sub\"].mean() * 100\n",
    "\n",
    "    # We want the mean score across all senses, as it does not mean a lot if a program can correctly define one big cluster containing most of the data and fail at all other senses.\n",
    "    percentage_weighted = percentage_default_mean.mean()\n",
    "    print(\"BIC:\")\n",
    "    print(\"Score for each\", percentage_default_mean)\n",
    "    print(\"Overall score\", percentage_default)    \n",
    "    \n",
    "    \n",
    "    \n",
    "    df.to_csv(f\"{model_name}/Experiment_2/Automatic/{keyword}.csv\", sep=\";\", encoding=\"utf-8\", index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-17T21:06:48.265910Z",
     "start_time": "2024-05-17T21:06:36.136779Z"
    }
   },
   "id": "e954509a273997d8",
   "execution_count": 16
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "e555dfbb839e9b16"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
