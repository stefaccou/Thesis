{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {},
   "outputs": [],
   "source": [
    "# origineel: aangepast om op CamemBERT te werken\n",
    "# !git clone https://github.com/jvladika/Lexical-Substitution.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "12d5d2c990307dbb",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-18T20:10:18.232602Z",
     "start_time": "2024-05-18T20:10:00.285344Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\stefa\\anaconda3\\envs\\Thesis\\Lib\\site-packages\\transformers\\utils\\generic.py:260: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, FlaubertModel, FlaubertWithLMHeadModel\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "from sklearn.mixture import GaussianMixture\n",
    "import warnings\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from french_lefff_lemmatizer.french_lefff_lemmatizer import FrenchLefffLemmatizer as fll\n",
    "import torch\n",
    "import string\n",
    "import nltk\n",
    "import time\n",
    "import numpy as np\n",
    "# Cuda\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('flaubert/flaubert_base_cased')\n",
    "lm_model = FlaubertWithLMHeadModel.from_pretrained('flaubert/flaubert_base_cased').to(device)\n",
    "raw_model = FlaubertModel.from_pretrained('flaubert/flaubert_base_cased', output_hidden_states=True, output_attentions=True).to(device)\n",
    "def load_transformers():\n",
    "    return tokenizer, lm_model, raw_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "70a6a28cb34345b2",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-18T20:10:18.254829Z",
     "start_time": "2024-05-18T20:10:18.235597Z"
    }
   },
   "outputs": [],
   "source": [
    "# imports from filter\n",
    "def filter_substitutions(substitutions):\n",
    "    dels = list()\n",
    "    for sub in substitutions:\n",
    "        if sub.lower() in substitutions and sub.capitalize() in substitutions:\n",
    "            dels.append(sub.capitalize())\n",
    "        if sub.lower() in substitutions and sub.upper() in substitutions:\n",
    "            dels.append(sub.upper())\n",
    "        if sub in nltk.corpus.stopwords.words('french') or sub in string.punctuation:\n",
    "            dels.append(sub)\n",
    "    dels = list(set(dels))\n",
    "    for d in dels:\n",
    "        substitutions.remove(d)\n",
    "    return substitutions\n",
    "\n",
    "def filter_words(target, words, scores, tokens):\n",
    "    # lets time\n",
    "    dels = list()\n",
    "    toks = tokens.tolist()\n",
    "    blacklist = [target, target.capitalize()]\n",
    "    \n",
    "    for w in words:\n",
    "        if w.lower() in words and w.capitalize() in words:\n",
    "            dels.append(w.capitalize())\n",
    "        if w.lower() in words and w.upper() in words:\n",
    "            dels.append(w.upper())\n",
    "        if w in nltk.corpus.stopwords.words('french') or w in string.punctuation:\n",
    "            dels.append(w)\n",
    "        if w in blacklist:\n",
    "            dels.append(w)\n",
    "    \n",
    "\n",
    "    dels = list(set(dels))\n",
    "    for d in dels:\n",
    "        del scores[words.index(d)]\n",
    "        del toks[words.index(d)]\n",
    "        words.remove(d)\n",
    "    \n",
    "\n",
    "    return words, scores, torch.tensor(toks)\n",
    "\n",
    "\n",
    "# imports from scores\n",
    "\n",
    "#Calculates the similarity score\n",
    "def similarity_score(original_output, subst_output, k):\n",
    "    mask_idx = k\n",
    "    cos_sim = torch.nn.CosineSimilarity(dim=1, eps=1e-6)\n",
    "    # Veranderen 3 naar 2\n",
    "    weights = torch.div(torch.stack(list(original_output[2])).squeeze().sum(0).sum(0), (12 * 12.0))\n",
    "\n",
    "    #Calculate the similarittimey score \n",
    "    #SIM(x, x'; k) = sum_i^L [ w_{i,k} * cos(h(x_i|x), h(x_i'|x')) ]\n",
    "\n",
    "    #subst_output = raw_model(sent.reshape(1, sent.shape[0]))\n",
    "    suma = 0.0\n",
    "    # Veranderen 2 naar 1\n",
    "    sent_len = original_output[1][2].shape[1]\n",
    "\n",
    "    for token_idx in range(sent_len): \n",
    "        # Veranderen 2 naar 1\n",
    "        original_hidden = original_output[1]\n",
    "        # Veranderen 2 naar 1\n",
    "        subst_hidden = subst_output[1]\n",
    "\n",
    "        #Calculate the contextualized representation of the i-th word as a concatenation of RoBERTa's values in its last four layers\n",
    "        context_original = torch.cat( tuple( [original_hidden[hs_idx][:, token_idx, :] for hs_idx in [1, 2, 3, 4]] ), dim=1)\n",
    "        context_subst = torch.cat( tuple( [subst_hidden[hs_idx][:, token_idx, :] for hs_idx in [1, 2, 3, 4]] ), dim=1)\n",
    "        suma += weights[mask_idx][token_idx] * cos_sim(context_original, context_subst)\n",
    "\n",
    "    substitute_validation = suma\n",
    "    return substitute_validation\n",
    "\n",
    "\n",
    "#Calculates the proposal score\n",
    "def proposal_score(original_score, subst_scores):\n",
    "    subst_scores = torch.tensor(subst_scores)\n",
    "    # we have to revert original_score to cpu\n",
    "    original_score = original_score.cpu()\n",
    "    return np.log(torch.div(subst_scores , (1.0 - original_score)) )\n",
    "\n",
    "\n",
    "#finals, props, subval = calc_scores(scores, input_ids[i], input_embeds[i], original_score, mask_position)\n",
    "def calc_scores(scr, sentences, original_output, original_score, mask_index):\n",
    "    #Get representations of all substitute sentences\n",
    "    sentences= torch.tensor(sentences).to(device)\n",
    "    subst_output = raw_model(sentences)\n",
    "\n",
    "    prop_score = proposal_score(original_score, scr) # this is cpu\n",
    "    substitute_validation = similarity_score(original_output, subst_output, mask_index)\n",
    "\n",
    "    alpha = 0.003\n",
    "    # Move prop_score to the same device as substitute_validation before the operation\n",
    "    prop_score = prop_score.to(substitute_validation.device)\n",
    "\n",
    "    final_score = substitute_validation + alpha*prop_score\n",
    "    \n",
    "    '''\n",
    "    print(\"Proposal score: \" + str(prop_score))\n",
    "    print(\"Subst. validation: \" + str(substitute_validation))\n",
    "    print(\"Final score for \" + str(final_score) + \"\\n\")\n",
    "    '''\n",
    "    return final_score, prop_score, substitute_validation\n",
    "\n",
    "WSD_PATTERN = r' (\\w+)/\\w+\\.\\w\\.\\d+' # (Woord), letterlijke slash, woord, letterlijke punt, letter, cijfer(s)\n",
    "\n",
    "def preproc_sentence(sentence):\n",
    "    sent_preproc = re.sub(WSD_PATTERN, r'\\1', sentence) # Alleen woord blijft over\n",
    "    return sent_preproc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cf443f5c962246d3",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-18T20:10:18.271470Z",
     "start_time": "2024-05-18T20:10:18.256829Z"
    }
   },
   "outputs": [],
   "source": [
    "def lexsub_dropout(sentence, target, topk=15):\n",
    "    sentence = sentence.replace('-', ' ')\n",
    "    table = str.maketrans(dict.fromkeys(string.punctuation)) \n",
    "\n",
    "    #Remove unnecessary punctuation from the sentence (such as: \"GET *free food *coupons!!\")\n",
    "    split_sent = nltk.word_tokenize(sentence)\n",
    "    split_sent = list(map(lambda wrd : wrd.translate(table) if wrd not in string.punctuation else wrd, split_sent))\n",
    "    original_sent = ' '.join(split_sent)\n",
    "\n",
    "    #Get raw model word embeddings for words in the sentence\n",
    "    original_token_input = tokenizer.encode(\" \"+original_sent, return_tensors=\"pt\").to(device)\n",
    "    original_output = raw_model(original_token_input)\n",
    "    # TVDC hidden_states zitten op index 1 ipv 2\n",
    "    inputs_embeds = original_output[1][1]\n",
    "\n",
    "    #The target word to substitute\n",
    "    target_token_id = tokenizer.encode(\" \"+target)[1]\n",
    "    input_ids = tokenizer.encode(\" \" + original_sent)\n",
    "    \n",
    "    mask_position = input_ids.index(target_token_id)\n",
    "    #input_ids = torch.tensor(input_ids).to(device)\n",
    "    #Set a percentage of randomly selected embedding weights of the target word to 0.\n",
    "    embedding_dim = 768\n",
    "    dropout_percent = 0.3\n",
    "    dropout_amount = round(dropout_percent*embedding_dim)\n",
    "\n",
    "    #Start timing the experiment.\n",
    "    #start = time.time()\n",
    "\n",
    "    #Run multiple experiments and then take average because of stochastic nature of choosing indices to dropout (sometimes the predictions are gibberish)\n",
    "    all_scores = dict()\n",
    "    all_counts = dict()\n",
    "    num_iterations = 5\n",
    "    for it in range(num_iterations):\n",
    "        #Choose the weight indices to drop out.\n",
    "        dropout_indices = np.random.choice(embedding_dim, dropout_amount, replace=False)\n",
    "        #Apply dropout to the embeddings\n",
    "        inputs_embeds[0, mask_position, dropout_indices] = 0\n",
    "        #Pass the embeddings where masked word's embedding is partially droppped out to the model \n",
    "        with torch.no_grad():\n",
    "                output = lm_model(inputs_embeds=inputs_embeds)\n",
    "        logits = output[0].squeeze()\n",
    "        #Get top guesses\n",
    "        mask_logits = logits[mask_position]\n",
    "        top_tokens = torch.topk(mask_logits, k=16, dim=0)[1]\n",
    "        scores = torch.softmax(mask_logits, dim=0)[top_tokens].tolist()\n",
    "        words = [tokenizer.decode(i.item()).strip() for i in top_tokens]\n",
    "        words, scores, top_tokens = filter_words(target, words, scores, top_tokens) # THIS IS WHAT MAKES IT SLOW\n",
    "        assert len(words) == len(scores)\n",
    "        if len(words) == 0: \n",
    "            continue\n",
    "\n",
    "        #Calculate proposal scores, substitute validation scores, and final scores\n",
    "        original_score = torch.softmax(mask_logits, dim=0)[target_token_id]\n",
    "        sentences = list()\n",
    "        for i in range(len(words)):\n",
    "            subst_word = top_tokens[i]\n",
    "            input_ids[mask_position] = int(subst_word)\n",
    "            sentences.append(list(input_ids))\n",
    "       \n",
    "        finals, props, subval = calc_scores(scores, sentences, original_output, original_score, mask_position)\n",
    "        finals = map(lambda f : float(f), finals)\n",
    "        #props = map(lambda f : float(f), props)\n",
    "        #subval = map(lambda f : float(f), subval)\n",
    "\n",
    "        if target in words:\n",
    "            words.remove(target)\n",
    "\n",
    "        #Update total scores and counts in the dictionary\n",
    "        res = dict(zip(words, finals))\n",
    "        for w, s in res.items():\n",
    "            all_scores[w] = all_scores[w] + s if w in all_scores.keys() else s\n",
    "            all_counts[w] = all_counts[w] + 1 if w in all_counts.keys() else 1\n",
    "    #Get the average of accumulated scores.\n",
    "    for w, s in all_scores.items():\n",
    "        all_scores[w] = s / all_counts[w]\n",
    "    words, finals = list(all_scores.keys()), list(all_scores.values())\n",
    "\n",
    "\n",
    "    #Sort the found substitutes by scores and print them out.\n",
    "    x = dict(zip(words, finals)) # list of words and final scores in dict form\n",
    "    sorted_list = list(sorted(x.items(), key=lambda item: item[1], reverse=True))[:topk] # take the \"topk\" best substitutes\n",
    "    #print([\"({0}: {1:0.8f})\".format(k, v) for k,v in sorted_list])\n",
    "    #print(\"Elapsed time: \", time.time() - start, \"\\n\")\n",
    "    return sorted_list\n"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "test = lexsub_dropout(\"Le chat gris est assis sur le tapis.\", \"chat\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-18T20:10:19.148236Z",
     "start_time": "2024-05-18T20:10:18.273468Z"
    }
   },
   "id": "eed70ea457cb5a21",
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "id": "5ea09ae958f3609a",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# We apply this method to the dataset"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "model_name = \"Flaubert\"\n",
    "all_predictions = {}"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-18T20:10:19.152769Z",
     "start_time": "2024-05-18T20:10:19.149235Z"
    }
   },
   "id": "d78ecb1b3acde6ff",
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running tirer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 606/606 [06:30<00:00,  1.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total of 2 skips for dataframe length 606\n",
      "running tour\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 582/582 [06:04<00:00,  1.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total of 22 skips for dataframe length 582\n",
      "running vol\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 586/586 [06:34<00:00,  1.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total of 2 skips for dataframe length 586\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "lemmatizer = fll()\n",
    "for filename in os.listdir(f\"Corpus/Final/Automatic/Part_3\"):\n",
    "    if filename.endswith(\".csv\"):\n",
    "        df_gs = pd.read_csv(f\"Corpus/Final/Manual/\" + filename, sep=\";\", encoding=\"utf-8\", header=0)\n",
    "        df_automatic = pd.read_csv(f\"Corpus/Final/Automatic/Part_3/\" + filename, sep=\";\", encoding=\"utf-8\", header=0)\n",
    "\n",
    "        df = pd.concat([df_gs, df_automatic], ignore_index=True)\n",
    "        # We calculate predictions\n",
    "        predictions = []\n",
    "        keyword = df[\"source\"][1].lower()\n",
    "        skips = 0\n",
    "        \n",
    "        print(\"running\", keyword)\n",
    "        sentences = df[\"match\"].values\n",
    "        if keyword == \"tour\":\n",
    "            sentences = [preproc_sentence(sentence) for sentence in sentences]\n",
    "        for sentence in tqdm(sentences):\n",
    "            # We do not apply the function if the literal keyword is not in the sentence\n",
    "            try:\n",
    "                substitutions = lexsub_dropout(sentence, keyword)\n",
    "                sentence_tuple = (sentence, substitutions)\n",
    "                predictions.append(sentence_tuple)\n",
    "            except:\n",
    "                #print(\"Could not find a substitution for \", sentence)\n",
    "                skips += 1\n",
    "                #print(\"total of\", skips, \"skips for dataframe length\", len(df))\n",
    "        print(\"total of\", skips, \"skips for dataframe length\", len(df))\n",
    "        all_predictions[keyword] = predictions\n",
    "        "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-18T20:29:30.809794Z",
     "start_time": "2024-05-18T20:10:19.153769Z"
    }
   },
   "id": "7347c22e42b370c3",
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# We save the predictions to a pandas dataframe (columns \"keyword\", 'sentence', 'predictions'\n",
    "if not os.path.exists(f\"{model_name}/Experiment_3/Automatic/predictions/\"):\n",
    "    os.makedirs(f\"{model_name}/Experiment_3/Automatic/predictions/\")\n",
    "for match, predictions in all_predictions.items():\n",
    "    df = pd.DataFrame(predictions, columns=[\"match\", \"predictions\"])\n",
    "    df[\"source\"] = match\n",
    "    # we add a 'match'\n",
    "    df.to_csv(f\"{model_name}/Experiment_3/Automatic/predictions/{match}.csv\", sep=\";\", encoding=\"utf-8\", index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-18T20:29:39.677915Z",
     "start_time": "2024-05-18T20:29:39.559934Z"
    }
   },
   "id": "4c43338b852fa9da",
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "df = pd.DataFrame(columns=[\"source\", \"match\", \"predictions\"])\n",
    "for match, predictions in all_predictions.items():\n",
    "    for sentence, subs in predictions:\n",
    "        new_row = pd.DataFrame({\"source\": [match], \"match\": [sentence], \"predictions\": [subs]})\n",
    "        df = pd.concat([df, new_row], ignore_index=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-18T20:29:40.524582Z",
     "start_time": "2024-05-18T20:29:39.679912Z"
    }
   },
   "id": "861192da02200917",
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# We then create a list of all the different words in \"forward\" and \"backward\"\n",
    "all_words = {}\n",
    "for match, predictions in all_predictions.items():\n",
    "    all_words[match] = []\n",
    "    for sentence, subs in predictions:\n",
    "        all_words[match].extend([prediction[0] for prediction in subs])\n",
    "    all_words[match] = list(set(all_words[match]))\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-18T20:29:40.542063Z",
     "start_time": "2024-05-18T20:29:40.530570Z"
    }
   },
   "id": "72b264034b69b1c5",
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "created dataframe for  tirer\n",
      "converted tirer into <class 'numpy.ndarray'>\n",
      "created dataframe for  tour\n",
      "converted tour into <class 'numpy.ndarray'>\n",
      "created dataframe for  vol\n",
      "converted vol into <class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "# Represent the sentences as sparse vectors\n",
    "# We do this by making a dataframe for each word: the index is the sentence, the columns are the words, and the values are the probabilities\n",
    "# We transform the dataframe\n",
    "vectors = {}\n",
    "for keyword, predictions in all_predictions.items():\n",
    "    df_sparse = pd.DataFrame(columns=[\"source\", \"match\"] + all_words[keyword])\n",
    "    print(\"created dataframe for \", keyword)\n",
    "    rows = []\n",
    "    for sentence, subs in predictions:\n",
    "        row = {\"source\": keyword, \"match\": sentence}\n",
    "        for word in all_words[keyword]:\n",
    "            row[word] = 0\n",
    "        for prediction in subs:\n",
    "            word, prob = prediction\n",
    "            row[word] = + prob  # We want to add to it if it already exists (forward and backward)\n",
    "        rows.append(row)\n",
    "    df_sparse = pd.DataFrame(rows)\n",
    "    vector = df_sparse.drop(columns=[\"source\", \"match\"]).values\n",
    "    # We now have a numpy ndarray, which we can transform using the TfidfTransformer\n",
    "    #transformer = TfidfTransformer()\n",
    "    #vector_Tfidf = transformer.fit_transform(vector)\n",
    "    finite_vals = vector[np.isfinite(vector)]\n",
    "    max_finite_val = np.max(finite_vals) if finite_vals.size > 0 else 1\n",
    "    vector[np.isinf(vector)] = max_finite_val\n",
    "    \n",
    "    # Replace NaNs with zero\n",
    "    vector[np.isnan(vector)] = 0\n",
    "    svd = TruncatedSVD(n_components=100)\n",
    "    vector_SVD = svd.fit_transform(vector)\n",
    "    vectors[keyword] = vector_SVD\n",
    "    print(f\"converted {keyword} into {type(vector)}\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-18T20:29:47.174657Z",
     "start_time": "2024-05-18T20:29:41.584805Z"
    }
   },
   "id": "f1843ab8a2d11e88",
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting model with 3 clusters\n",
      "Fitting model with 4 clusters\n",
      "Fitting model with 5 clusters\n",
      "Fitting model with 6 clusters\n",
      "Fitting model with 7 clusters\n",
      "Fitting model with 8 clusters\n",
      "Fitting model with 9 clusters\n",
      "Fitting model with 10 clusters\n",
      "Optimal number of clusters: 5\n",
      "agg:\n",
      "Score for each sense\n",
      "pull      3.333333\n",
      "shoot    97.297297\n",
      "Name: agg_default_sub, dtype: float64\n",
      "Overall score 70.1923076923077\n",
      "BIC:\n",
      "Score for each sense\n",
      "pull     40.000000\n",
      "shoot    52.702703\n",
      "Name: BIC_default_sub, dtype: float64\n",
      "Overall score 49.03846153846153\n",
      "Fitting model with 3 clusters\n",
      "Fitting model with 4 clusters\n",
      "Fitting model with 5 clusters\n",
      "Fitting model with 6 clusters\n",
      "Fitting model with 7 clusters\n",
      "Fitting model with 8 clusters\n",
      "Fitting model with 9 clusters\n",
      "Fitting model with 10 clusters\n",
      "Optimal number of clusters: 3\n",
      "agg:\n",
      "Score for each sense\n",
      "lap        0.000000\n",
      "round      0.000000\n",
      "tower      0.000000\n",
      "trick      7.142857\n",
      "turn     100.000000\n",
      "Name: agg_default_sub, dtype: float64\n",
      "Overall score 33.78378378378378\n",
      "BIC:\n",
      "Score for each sense\n",
      "lap      31.250000\n",
      "round     7.142857\n",
      "tower     0.000000\n",
      "trick     0.000000\n",
      "turn     50.000000\n",
      "Name: BIC_default_sub, dtype: float64\n",
      "Overall score 24.324324324324326\n",
      "Fitting model with 3 clusters\n",
      "Fitting model with 4 clusters\n",
      "Fitting model with 5 clusters\n",
      "Fitting model with 6 clusters\n",
      "Fitting model with 7 clusters\n",
      "Fitting model with 8 clusters\n",
      "Fitting model with 9 clusters\n",
      "Fitting model with 10 clusters\n",
      "Optimal number of clusters: 10\n",
      "agg:\n",
      "Score for each sense\n",
      "flight    93.333333\n",
      "theft      0.000000\n",
      "Name: agg_default_sub, dtype: float64\n",
      "Overall score 65.88235294117646\n",
      "BIC:\n",
      "Score for each sense\n",
      "flight    18.333333\n",
      "theft     24.000000\n",
      "Name: BIC_default_sub, dtype: float64\n",
      "Overall score 20.0\n"
     ]
    }
   ],
   "source": [
    "# Initialize the clustering algorithm\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "# 'n_clusters' is the number of clusters we want to form (and also the number of clusters to be found)\n",
    "# 'linkage' is the linkage criterion (can be 'ward', 'complete', 'average', 'single')\n",
    "clustermin = 3\n",
    "\n",
    "agg_clustering = AgglomerativeClustering(n_clusters=clustermin, metric='cosine', linkage='average')\n",
    "\n",
    "# Range of potential cluster numbers to test\n",
    "cluster_range = range(clustermin, 11)\n",
    "\n",
    "for keyword, vector_SVD in vectors.items():\n",
    "    df = pd.read_csv(f\"{model_name}/Experiment_1/Curated/{keyword}.csv\", sep=\";\", encoding=\"utf-8\", header=0)\n",
    "    df_gs = pd.read_csv(f\"Corpus/Final/Manual/{keyword}.csv\", sep=\";\", encoding=\"utf-8\", header=0)\n",
    "    df_automatic = pd.read_csv(f\"Corpus/Final/Automatic/{keyword}.csv\", sep=\";\", encoding=\"utf-8\", header=0)\n",
    "    df_together = pd.concat([df_gs, df_automatic], ignore_index=True)\n",
    "    # Apply the clustering algorithm to the SVD-transformed vectors\n",
    "    agg_clusters = agg_clustering.fit_predict(vector_SVD)\n",
    "\n",
    "    # 'agg_clusters' is now an array where the i-th element is the cluster label of the i-th instance\n",
    "    # We add \"1\" to all clusters to start counting at 1 instead of 0\n",
    "    agg_clusters += 1\n",
    "    # We add the cluster label to the original dataframe, corresponding to the correct index\n",
    "    agg_sentence_cluster_dict = dict(zip(df_together[\"match\"], agg_clusters))\n",
    "    df[\"agg_cluster_sub\"] = df[\"match\"].map(agg_sentence_cluster_dict)\n",
    "\n",
    "    # List to hold BIC values\n",
    "    bic_values = []\n",
    "\n",
    "    # Fit Gaussian Mixture Models for each number of clusters\n",
    "    for i in cluster_range:\n",
    "        print(f\"Fitting model with {i} clusters\")\n",
    "        gmm = GaussianMixture(n_components=i, random_state=0).fit(vector_SVD)\n",
    "        bic_values.append(gmm.bic(vector_SVD))\n",
    "\n",
    "    # Find the number of clusters that gives the minimum BIC\n",
    "    optimal_clusters = cluster_range[np.argmin(bic_values)]\n",
    "    print(f\"Optimal number of clusters: {optimal_clusters}\")\n",
    "\n",
    "    # Fit the optimal model\n",
    "    gmm_optimal = GaussianMixture(n_components=optimal_clusters).fit(vector_SVD)\n",
    "\n",
    "    # Predict the cluster for each data point\n",
    "    BIC_clusters = gmm_optimal.predict(vector_SVD)\n",
    "    # We want them to start counting at \"1\" instead of \"0\"\n",
    "    BIC_clusters += 1\n",
    "    # We now map these clusters to the original dataframe, for their respective \"sentence\"\n",
    "    BIC_sentence_cluster_dict = dict(zip(df_together[\"match\"], BIC_clusters))\n",
    "    # Map the cluster ids to the sentences in the dataframe\n",
    "    df['BIC_cluster_sub'] = df['match'].map(BIC_sentence_cluster_dict)\n",
    "\n",
    "    # We calculate a score for the clustering scores (starting before the outliers are removed)\n",
    "    # Group the dataframe by \"sense\" and \"cluster\", and calculate the size of each group\n",
    "    df_grouped = df.groupby([\"sense\", \"agg_cluster_sub\"]).size().reset_index(name=\"count\")\n",
    "\n",
    "    # Sort these clusters by size in descending order\n",
    "    df_grouped = df_grouped.sort_values(by=\"count\", ascending=False)\n",
    "\n",
    "    # Initialize an empty dictionary to store the cluster numbers that have been assigned as default clusters\n",
    "    # If the cluster number is not taken, we assign it to the corresponding \"sense\"\n",
    "    # Else, we try to assign it to the next cluster number\n",
    "    cluster_dict = {}\n",
    "    for index, row in df_grouped.iterrows():\n",
    "        if row[\"sense\"] not in cluster_dict:\n",
    "            if row[\"agg_cluster_sub\"] not in cluster_dict.values():\n",
    "                cluster_dict[row[\"sense\"]] = row[\"agg_cluster_sub\"]\n",
    "\n",
    "        # We add \"sense\" values that have no entry in cluster_dict and set value to 0 (always seen as wrong)\n",
    "    for sense in df[\"sense\"].unique():\n",
    "        if sense not in cluster_dict:\n",
    "            cluster_dict[sense] = 0\n",
    "\n",
    "    # Add a new column \"default\" to the original dataframe\n",
    "    df[\"agg_default_sub\"] = df.apply(lambda x: x[\"agg_cluster_sub\"] == cluster_dict[x[\"sense\"]], axis=1)\n",
    "\n",
    "    # We calculate the percentage of default clusters\n",
    "    percentage_default = (df[\"agg_default_sub\"].sum() / len(df)) * 100\n",
    "    # We also calculate this separately for each \"sense\"\n",
    "    percentage_default_mean = df.groupby(\"sense\")[\"agg_default_sub\"].mean() * 100\n",
    "\n",
    "    # We want the mean score across all senses, as it does not mean a lot if a program can correctly define one big cluster containing most of the data and fail at all other senses.\n",
    "    percentage_weighted = percentage_default_mean.mean()\n",
    "    print(\"agg:\")\n",
    "    print(\"Score for each\", percentage_default_mean)\n",
    "    print(\"Overall score\", percentage_default)\n",
    "\n",
    "    #----------------------------------------------------------------------------------------------------\n",
    "    # We do this again for BIC clusters\n",
    "    #----------------------------------------------------------------------------------------------------\n",
    "\n",
    "    # We calculate a score for the clustering scores (starting before the outliers are removed)\n",
    "    # Group the dataframe by \"sense\" and \"cluster\", and calculate the size of each group\n",
    "    df_grouped = df.groupby([\"sense\", \"BIC_cluster_sub\"]).size().reset_index(name=\"count\")\n",
    "\n",
    "    # Sort these clusters by size in descending order\n",
    "    df_grouped = df_grouped.sort_values(by=\"count\", ascending=False)\n",
    "\n",
    "    # Initialize an empty dictionary to store the cluster numbers that have been assigned as default clusters\n",
    "    # If the cluster number is not taken, we assign it to the corresponding \"sense\"\n",
    "    # Else, we try to assign it to the next cluster number\n",
    "    cluster_dict = {}\n",
    "    for index, row in df_grouped.iterrows():\n",
    "        if row[\"sense\"] not in cluster_dict:\n",
    "            if row[\"BIC_cluster_sub\"] not in cluster_dict.values():\n",
    "                cluster_dict[row[\"sense\"]] = row[\"BIC_cluster_sub\"]\n",
    "\n",
    "        # We add \"sense\" values that have no entry in cluster_dict and set value to 0 (always seen as wrong)\n",
    "    for sense in df[\"sense\"].unique():\n",
    "        if sense not in cluster_dict:\n",
    "            cluster_dict[sense] = 0\n",
    "\n",
    "    # Add a new column \"default\" to the original dataframe\n",
    "    df[\"BIC_default_sub\"] = df.apply(lambda x: x[\"BIC_cluster_sub\"] == cluster_dict[x[\"sense\"]], axis=1)\n",
    "\n",
    "    # We calculate the percentage of default clusters\n",
    "    percentage_default = (df[\"BIC_default_sub\"].sum() / len(df)) * 100\n",
    "    # We also calculate this separately for each \"sense\"\n",
    "    percentage_default_mean = df.groupby(\"sense\")[\"BIC_default_sub\"].mean() * 100\n",
    "\n",
    "    # We want the mean score across all senses, as it does not mean a lot if a program can correctly define one big cluster containing most of the data and fail at all other senses.\n",
    "    percentage_weighted = percentage_default_mean.mean()\n",
    "    print(\"BIC:\")\n",
    "    print(\"Score for each\", percentage_default_mean)\n",
    "    print(\"Overall score\", percentage_default)\n",
    "\n",
    "    df.to_csv(f\"{model_name}/Experiment_3/Automatic/{keyword}.csv\", sep=\";\", encoding=\"utf-8\", index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-18T20:29:57.545699Z",
     "start_time": "2024-05-18T20:29:54.234663Z"
    }
   },
   "id": "7c8bbf9e123673b3",
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "a7c35849c345208c"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
